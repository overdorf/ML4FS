{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631b1ff2",
   "metadata": {},
   "source": [
    "# TP 2: Pandas\n",
    "\n",
    "## Introduction\n",
    "Pandas is a Python library that helps you manipulate structured data efficiently. The goal of pandas is to permit real world data analysis with Python.\n",
    "\n",
    "Python has natively data structures such as lists, tuples, dictionaries and sets. Pandas introduces two new data structures: Series and DataFrames. \n",
    "\n",
    "A DataFrame is essentially a Table you can manipulate in Python. It has columns, rows and an index. Many operations in pandas can either be applied row-vise, which you can specify by passing the argument axis=0, or column-wise, which you can specify by passing the argument axis=1.\n",
    "\n",
    "Visualisation aid:\n",
    "|index | column_a | ... | column_z \n",
    "| --- | --- | --- |--- |\n",
    "| 1 | value_1 | ... | value_t\n",
    "| 2 | value_2 | ... | value_s\n",
    "| ... | ... | ... |\n",
    "| n | value_m | ... | value_z\n",
    "\n",
    "A Series can be visualized as a Table with a single column.\n",
    "\n",
    "Visualisation aid:\n",
    "|index | |\n",
    "| --- | --- |\n",
    "| 1 | value_1 | \n",
    "| 2 | value_2 |\n",
    "| ... | ... |\n",
    "| n | value_n |\n",
    "\n",
    "In this TP you will learn to manipulate Dataframes by solving a little murder mystery.\n",
    "\n",
    "A useful resource is the official pandas documentation, which you can access here: https://pandas.pydata.org/docs/\n",
    "It is encouraged to read the documentation, and to reference it in future while you do your projects. There might be useful functionalities we do not have the time to present you today.\n",
    "\n",
    "## Scenario\n",
    "On the 23.12.2023 at 05:09 AM the body of Hayden Hill is discovered in a cleaning cabinet in Room 4 on the 3rd floor of the office building from Evil Incorporate. The body was discovered by a member of the cleaning service. The cause of death was determined to by manual strangulation, and the crime scene specialists determined that the body likely hadn't been moved. The time of death is estimated to be between 23:00 and 03:00 on the night of the 22nd to the 23rd of december 2023. \n",
    "The night before, the company Christmas took place in the building. Plus ones were permitted to attend the party, but due to the security system, any plus ones had a visitor badge for the night of the Christmas party.\n",
    "The security system logs access to a room but not at what time the badge-holder leaves the room.\n",
    "\n",
    "The lead investigator has tasked you with reducing the suspect pool based on the logfile of the access badges.\n",
    "\n",
    "You are provided with 3 csv files: \n",
    "- logfile_20231222.csv and logfile_20231223.csv, containing all badge uses for the office building the body was found in.\n",
    "- batch_data.csv, containing the information on the key badges and their holders.\n",
    "\n",
    "## Step 1: Load the data\n",
    "Load the data from logfile_20231222.csv as well as from logfile_20231223.csv into a DataFrame object.\n",
    "\n",
    "There are many different ways to load data into a Dataframe, but for this TP we work with csvs.\n",
    "\n",
    "Because the logfiles are split into two files, we need to merge the two Dataframes.\n",
    "For this we use pd.concat https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "The function pd.concat adds the rows of two dataframes together if used with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6369489d39da285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jwyss2\\appdata\\local\\miniconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\jwyss2\\appdata\\local\\miniconda3\\lib\\site-packages (from pandas) (1.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jwyss2\\appdata\\local\\miniconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jwyss2\\appdata\\local\\miniconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jwyss2\\appdata\\local\\miniconda3\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jwyss2\\appdata\\local\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing the packages \n",
    "# (in jupyter notebooks, the line starting with ! are equivalent to writing them in a Terminal Window, so the following line will install the package)\n",
    "!pip3 install pandas \n",
    "# if you have a Warning with pandas (DepreciationWarning) you can install pyarrow with the following line (just uncomment it):\n",
    "#!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585ece90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all libraries needed today\n",
    "import pandas as pd \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32608d61",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3264638813.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    df_log_23 =  ###Insert your code here\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Load the two csv into a Dataframe. Hint: There is a specific function from Panda that will permit this\n",
    "df_log_22 =  ###Insert your code here\n",
    "df_log_23 =   ###Insert your code here\n",
    "\n",
    "#Step 2: Combine the Dataframes with the function previously mentioned\n",
    "df_log =  ###Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30996fa",
   "metadata": {},
   "source": [
    "## Step 2: Explore your dataframe\n",
    "Now that you have loaded the data, the next step is to have a general idea of what your data looks like.\n",
    "The most basic functions that permit this are:\n",
    "\n",
    "1. df.head(n) & df.tail(n)\n",
    "\n",
    "These two functions give you the first respectively last n entries in your dataframe. The default number of entries shown is 5.\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html\n",
    "\n",
    "2. df.shape\n",
    "\n",
    "This returns a tuple with the number of rows and the number of columns in your table.\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html\n",
    "\n",
    "3. df.columns\n",
    "\n",
    "This returns the labels of the columns of the table.\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html\n",
    "\n",
    "4. df.index\n",
    "\n",
    "This returns the labels of the rows of the table.\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.index.html\n",
    "\n",
    "5. df.dtypes\n",
    "\n",
    "This returns a Series with the data type of each column. The result’s index is the original DataFrame’s columns. Columns with mixed types are stored with the object dtype. See the User Guide for more.\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html\n",
    "\n",
    "6. df.value_counts\n",
    "\n",
    "This function permits to see all distinct values\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html\n",
    "\n",
    "7. df.column_name.unique\n",
    "\n",
    "This function permit to see the different values that can be encountered in a given column\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.unique.html\n",
    "\n",
    "#### Selecting a column\n",
    "If you want to only select a specific column (just like in the example n° 7 above) of the Dataframe you can call it with the following syntax:\n",
    "\n",
    "df[column_name] or df.column_name\n",
    "\n",
    "You can read this as 'column_name from dataframe df'. If you only want to read information from a single column, both of these syntax are equivalent. However, for more complex requests (f.ex. wanting multiple rows), the df[column_name] syntax is better suited. This is because df[column_name] gives you full access, whereas df.column_name only gives you attribute access (https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#attribute-access). \n",
    "\n",
    "To select multiple columns, you just have to do it like this: df[column_name1, column_name2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c084f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Answer the questions below\n",
    "#How many rows does the Dataframe df_log have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the first 7 entries of the Dataframe df_log?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ec46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the type of entry each column of df_log?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d030fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What column labels does the Dataframe df_log have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e87d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many different rooms does the building have?\n",
    "#Hint: How many distinct values does the column door of the Dataframe df_log have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435118d",
   "metadata": {},
   "source": [
    "## Step 3: Drop empty & non-pertinent entries from dataframe\n",
    "Your Dataframe has some erroneous entries that does not contain any information.\n",
    "Empty entries can cause issues, and thus need to be removed.\n",
    "\n",
    "Luckily, there are multiple different functions to drop/modify empty values.\n",
    "\n",
    "1. df.drop\n",
    "The most general function, if you want to drop specific columns, rows...\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
    "2. df.dropna\n",
    "Function to drop n/A values\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "3. df.drop_duplicates\n",
    "Function to get rid of duplicate entries\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html\n",
    "4. df.fillna\n",
    "Function to replace empty values\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\n",
    "5. df.isna\n",
    "Function that creates a dataframe containing boolean values indicating whether an entry is empty or not.\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html\n",
    "\n",
    "### Note: Sometimes it's important to work on copies / Inplace\n",
    "We want to make an operation that modifies the Dataframe, without modifying the original data.\n",
    "Which is to say, you are interested in the result of the operation, but you don't want the process of calculating the result changing the input Dataframe.\n",
    "There are three ways to achieve this:\n",
    "\n",
    "1. Inbuilt modification-protection using the parameter inplace\n",
    "Many panda functions have the parameter inplace. It is by default set to False, which assures the operation doesn't modify input the Dataframe and returns a new object.\n",
    "\n",
    "2. df.copy\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html\n",
    "To achieve modification-protection using copy, you need to first make a copy of the dataframe of interest, and then proceed to use the copy for the operations you want to apply.\n",
    "\n",
    "#### Proceed with Caution: Memory Trade-Off\n",
    "Copying a dataframes means that you now have a duplicate of your dataframe in memory. Depending on how big your dataframe is, this can cause issue with further computations!\n",
    "\n",
    "3. using assignment: df_new = df_old[[some subset]]\n",
    "To achieve modification-protection using assignment, you need to first assign the dataframe (or a subset of columns or rows) to a new object, and then proceed to use the new object for the operations of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3627827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_demo (12167, 3)\n",
      "Shape of df_demo after dropping N/A (12167, 3)\n",
      "*** ASSIGNING RETURN VALUE TO ANOTHER DATAFRAME ***\n",
      "Shape of df_demo (12167, 3)\n",
      "Shape of df_copy (12163, 3)\n",
      "*** USING INPLACE ***\n",
      "Shape of df_copy2 without inplace=False (12167, 3)\n",
      "Shape of df_copy2 with inplace=True (12163, 3)\n"
     ]
    }
   ],
   "source": [
    "df_demo = pd.read_csv('logfile_20231222.csv')\n",
    "print(f\"Shape of df_demo {df_demo.shape}\")\n",
    "# dropping N/A on the df_demo won't change the dataframe\n",
    "df_demo.dropna()\n",
    "print(f\"Shape of df_demo after dropping N/A {df_demo.shape}\")\n",
    "print(\"*** ASSIGNING RETURN VALUE TO ANOTHER DATAFRAME ***\")\n",
    "#The first way of assuring the result of the operation is stored is to assign the return value to another dataframe. \n",
    "#With this method, the df_demo dataframe is untouched\n",
    "df_copy = df_demo.dropna()\n",
    "print(f\"Shape of df_demo {df_demo.shape}\")\n",
    "print(f\"Shape of df_copy {df_copy.shape}\")\n",
    "\n",
    "print(\"*** USING INPLACE ***\")\n",
    "#The second is using inplace parameters correctly. Inplace parameter will modify the dataframe on which the method is called, so to keep the original data untouched, we will first make a copy of the dataframe (duplicate it).\n",
    "# KEEP IN MIND: it is good practice to make a copy of original data to get a \"work copy\", but you don't need to do it at each step, especially if you have a big dataset\n",
    "df_copy2 = df_demo.copy(deep=True)\n",
    "df_copy2.dropna(inplace=False)\n",
    "print(f\"Shape of df_copy2 without inplace=False {df_copy2.shape}\")\n",
    "#Try to drop all NaN entries using inplace\n",
    "df_copy2.dropna(inplace=True)\n",
    "print(f\"Shape of df_copy2 with inplace=True {df_copy2.shape}\")\n",
    "\n",
    "#Inplace-misuse is a recurring source of errors while using pandas.\n",
    "#If you're functions don't do what they are supposed to, check if inplace is correctly used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76be0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Answer the questions below (concerning df_log)\n",
    "#Hint: It might be useful to read the next section on filtering to be able to solve some of the questions below\n",
    "#How many rows contain empty values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2534ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you apply dropna on the whole dataframe do you loose pertinent information?\n",
    "#Visualize the NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc93b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you loose pertinent information how do you combat this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b463e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many rows are duplicates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b8b68",
   "metadata": {},
   "source": [
    "## Step 4a: Make the data useful - Filter\n",
    "There are multiple ways to select a subset of a dataframe based on a condition/multiple conditions.\n",
    "\n",
    "### 1. Using comparison operators (>, =, >=, <=, !=)\n",
    "Example with one condition:\n",
    "\n",
    "Option 1:\n",
    "df_subset = df[df['column_name'] > minimal_value] \n",
    "#This returns a dataframe\n",
    "You can read df[df['column_name'] > minimal_value] as \"return all rows from dataframe df where the entry of column 'column_name' is bigger than 'minimal_value'\"\n",
    "\n",
    "Option 2:\n",
    "df_subset = df.loc[df['column_name'] > minimal_value]\n",
    "#Depending on how the [] are used this can return an index, a series or a dataframe\n",
    "To read up on .loc: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\n",
    "\n",
    "Example with multiple conditions:\n",
    "\n",
    "The principle is that conditions can be added using the & (read: 'AND') operator or the | (read: 'OR') operator.\n",
    "\n",
    "Option 1:\n",
    "df_subset = df[(df['column1'] > minimal_value) & (df['column2'] = other_value)]\n",
    "\n",
    "Option 2:\n",
    "df_subset = df.loc[(df['column1'] > minimal_value) & (df['column2'] = other_value)]\n",
    "\n",
    "### 2. Using df.isin()\n",
    "Documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html\n",
    "\n",
    "### 3. Using df.query()\n",
    "Documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410cabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Christmas party took place between 7pm and 1am and that the victim was last reported to be seen at 9pm giving a speech to the whole of the company.\n",
    "#Considering given elements, filter out the non-pertinent log-entries based on the time of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1e078",
   "metadata": {},
   "source": [
    "## Step 4b: Make the data useful - Applying functions\n",
    "If you want to apply a function to the entries of your dataframe, you can use df.apply().\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\n",
    "\n",
    "For example, if you want to apply a function to column_a and store the result in column_b the basic syntax would be:\n",
    "df['column_b'] = df['column_a'].apply(lambda x: some_function(x))\n",
    "You can read \"df['column_b'] = df['column_a'].apply(lambda x: some_function(x))\" as \"create a new column named column_b where the value for a given row is the result of the function some_function() is applied to the value in column_a of said row(s)\".\n",
    "\n",
    "#### Functions: How to (not) use lambda x\n",
    "If you use apply there are multiple ways you can pass the function.\n",
    "1. If the function is already defined elsewhere...\n",
    "\n",
    "    a. and only takes one argument\n",
    "You can use apply directly: df['column_b'] = df['column_a'].apply(function)\n",
    "\n",
    "    b. and takes multiple arguments\n",
    "You use lambda: df['column_b'] = df['column_a'].apply(lambda x: function(x, other_argument1, other_argument2))\n",
    "\n",
    "2. If the function is not yet defined...\n",
    "You use lambda to define the function directly: df['column_b'] = df['column_a'].apply(lambda x: (x+20)/15)\n",
    "\n",
    "A more sophisticated version of apply is groupby. You will not need it in this TP but for future reference, if you need to split the dataframe, apply the function to part of the data and then recombine the result, it would be worth checking groupby out:\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n",
    "\n",
    "If you know pivot tables from Excel, and you want to replicate their functionalities, this is possible with the function pivot_table.\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html\n",
    "\n",
    "### Already implemented functions\n",
    "For arithmetics there are already pre-written functions:\n",
    "1. add\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.add.html\n",
    "2. subtract\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sub.html#pandas.DataFrame.sub\n",
    "3. multiply\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mul.html#pandas.DataFrame.mul\n",
    "4. division\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.div.html\n",
    "5. modulo \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mod.html#pandas.DataFrame.mod\n",
    "6. exponential\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pow.html\n",
    "\n",
    "If you want to compare dataframes/columns, there are also pre-written functions:\n",
    "1. Equality: \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.eq.html\n",
    "2. Inequality: \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ne.html#pandas.DataFrame.ne\n",
    "3. less than or equal: \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.le.html#pandas.DataFrame.le\n",
    "4. less than:\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.lt.html#pandas.DataFrame.lt\n",
    "5. greater than or equal:\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ge.html#pandas.DataFrame.ge\n",
    "6. greater than: \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.gt.html#pandas.DataFrame.gt\n",
    "\n",
    "There are also functions to describe the values your dataframe/series contains:\n",
    "1. sum\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.sum.html\n",
    "2. mean\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html\n",
    "3. median\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.median.html\n",
    "4. smallest (min) entry\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.min.html\n",
    "5. biggest (max) entry\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.max.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d7dc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The room's IDs are in the format FF-RR (FF is floor number and RR is room number): \n",
    "#One number corresponding to the floor, the other to the room. They are separated by a hyphen ('-')\n",
    "#Create two new columns: one containing the floor information and another the room information.\n",
    "\n",
    "#Create a function to extract the information of interest and returns it as an int type\n",
    "def get_floor_and_room(string):\n",
    "    #Insert your code here\n",
    "    return floor, room\n",
    "\n",
    "#Insert the information of interest into the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e337e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now convert the column timestamp into a human-readable timestamp, it is a UNIX EPOCH timestamp, already in the correct timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e997d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m timestamp\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1500\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#We are in UTC +1:00, thus your friend wrote this code to apply the function and tell you to run the cell twice:\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_copy\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(), df_log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     14\u001b[0m df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: adjust_timezone_by_30min(x, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_copy' is not defined"
     ]
    }
   ],
   "source": [
    "#Sidequest: Smart cell-execution\n",
    "\n",
    "#You want to be able to adjust the timezone.\n",
    "#One of your friends passes you this function, and just tells you to apply it as many times as necessary.\n",
    "def adjust_timezone_by_30min(timestamp, plus=True):\n",
    "    \"\"\"Adds or subtracts the number of seconds of half an hour\"\"\"\n",
    "    if plus:\n",
    "        return timestamp+1500\n",
    "    else:\n",
    "        return timestamp-1500\n",
    "\n",
    "#We are in UTC +1:00, thus your friend wrote this code to apply the function and tell you to run the cell twice:\n",
    "print(df_copy['timestamp'].head(), df_log['timestamp'])\n",
    "df_copy['timestamp'] = df_copy['timestamp'].apply(lambda x: adjust_timezone_by_30min(x, True))\n",
    "print(df_copy['timestamp'].head())\n",
    "\n",
    "#List all the advantages and especially disadvantages of this method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8001fe",
   "metadata": {},
   "source": [
    "## Step 4c: Make the data useful - One-hot-encoding\n",
    "One-hot-encoding is a way to transform qualitative data into quantitative data.\n",
    "\n",
    "As a quick example:\n",
    "You have a survey where participants could reply to a question with good, neutral, bad. \n",
    "Your initial dataframe looks like this:\n",
    "|participantID| Question 1 | column_a |\n",
    "| --- | --- | --- |\n",
    "| 1 | Good | ... |\n",
    "| 2 | Good | ... |\n",
    "| 3 | Good | ... |\n",
    "| 4 | Bad | ... |\n",
    "| 5 | Good | ... |\n",
    "| 6 | Neutral | ... |\n",
    "\n",
    "With one-hot encoding your goal is to create new columns encoding the different answer possibilities.\n",
    "|participantID| Question 1 - Reply: Good | Question 1 - Reply: Neutral | Question 1 - Reply: Bad | column_a |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 1 | 1 | 0| 0 | ... |\n",
    "| 2 | 1 | 0| 0 | ... |\n",
    "| 3 | 1 | 0| 0 | ... |\n",
    "| 4 | 0 | 0| 1 | ... |\n",
    "| 5 | 1 | 0 | 0 | ... |\n",
    "| 6 | 0 | 1 | 0 |... |\n",
    "\n",
    "This is useful for example if you are using a machine learning algorithm that cannot cope with qualitative data.\n",
    "\n",
    "There is a pre-made function that performs one-hot encoding\n",
    "\n",
    "pd.get_dummies\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
    "\n",
    "as well as one to reverse one-hot encoding\n",
    "\n",
    "pd.from_dummies\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.from_dummies.html#pandas.from_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "032e6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To test one-hot encoding you need to import the csv called batch_user.csv\n",
    "df_id =#insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee445486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The column status has qualitative values, how many different values are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae5254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot-encode the 'status' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82055e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many 'visitor' badges are in use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ac579",
   "metadata": {},
   "source": [
    "## Step 4d: Make the data useful - Combine dataframes\n",
    "We now have two dataframes, one containing data about the visitor badges (df_id), one containing the one-hot-encoded version of status.\n",
    "For further use we want to combine them.\n",
    "\n",
    "There are multiple ways to combine dataframes. Here we present you the most popular one:\n",
    "\n",
    "pd.concat\n",
    "You used this function to combine rows, but it can also add-on columns. However, for this to work properly the dataframes need to share an identical index.\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "\n",
    "There are other functions for combining dataframes, be sure to check out the documentation in case concat does not work for the specific problem you encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8b5b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine df_id_dummies and df_id (add the columns of df_id to df_dummies in one dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "903bfd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recap question: Are there empty values in df?\n",
    "#Does it make sense to drop them?\n",
    "#Treat them in an appropriate way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb368d",
   "metadata": {},
   "source": [
    "## Step 4e: Make the data useful - Sorting\n",
    "Sometimes it's interesting to reorder the dataframe entries based on a different value than the index.\n",
    "df.sort_values is a useful tool for this.\n",
    "You can also sort based on multiple columns!\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00cee643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the dataframe df_copy (the one with the logs) based on the column it makes the most sense of sorting it by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590b6bd",
   "metadata": {},
   "source": [
    "## Step 5: Export your results\n",
    "You can save your dataframe to a file of (almost) any type.\n",
    "In this TP we show you how to save it as a csv, but in the documentation you will find other file types being supported.\n",
    "\n",
    "One thing to remember when saving a dataframe to a csv is that you might not want to save the index. If you save the csv with the index, when you later reload the csv into a dataframe the 'old' index will be added as a new column. Thus, be sure to save the index only when it makes sense to preserve it.\n",
    "\n",
    "df.to_csv(filename)\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "295b3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save df_copy to a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df20464",
   "metadata": {},
   "source": [
    "## Solving the mystery\n",
    "Use the skills learned in this TP to answer the following questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3724578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final questions - You can check these answers on moodle\n",
    "#Make a dataframe only depicting the movements of the victim and save if as a csv:\n",
    "\n",
    "#When did the victim go into the room of interest for the last time?\n",
    "\n",
    "#How many different keycards were used to open the room where the body was found?\n",
    "\n",
    "#What are the badge_ids that accessed the room the victim was found in during the timeframe the victim also accessed it (after midnight local)?\n",
    "\n",
    "#Make a dataframe describing the movements of suspect-badges after midnight:\n",
    "\n",
    "#Is there any suspect-badge(s) whose holders questioning should be prioritized?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
